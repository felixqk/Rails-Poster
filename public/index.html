<!DOCTYPE html> 
<html> 
<head> 
	<title>High-Quality Kinect Depth Filtering for Real-Time 3D Telepresence</title> 
	
	<meta name="viewport" content="width=device-width, initial-scale=1"> 
  <link href="https://fonts.googleapis.com/css?family=Karla:400,700" rel="stylesheet" type="text/css">
  <style>

  body {
    margin-left: 10%;
    margin-right: 10%;
  }

  img {
    display: block;
    margin-left: auto;
    margin-right: auto;
  }

  </style>

</head> 

<body> 
  <h1>High-Quality Kinect Depth Filtering for Real-Time 3D Telepresence</h1>
  <br/>

  <b>Mengyao Zhao, Fuwen Tan, Chi-Wing Fu, Chi-Keung Tang, Jianfei Cai, Tat Jen Cham</b>
  <br/>
  <br/>
  <b>Abstract</b>

  <p>3D telepresence is a next-generation multimedia application, offering remote users an immersive and natural video-conferencing environment with real-time 3D graphics. Kinect sensor, a consumer-grade range camera, facilitates the implementation of some recent 3D telepresence systems. However, conventional data filtering methods are insufficient to handle Kinect depth error because such error is quantized rather than just randomly-distributed. Hence, one could often observe large irregularly-shaped patches of pixels that receive the same depth values from Kinect. To enhance visual quality in 3D telepresence, we propose a novel depth data filtering method for Kinect by means of multi-scale and direction-aware support windows. In addition, we develop a GPU-based CUDA implementation that can perform real-time depth filtering. Results from the experiments show that our method can reconstruct hole-free surfaces that are smoother and less bumpy compared to existing methods like bilateral filtering.</p>

  <br>
  
  <b>Introduction</b>
    <p>3D telepresence is a next-generation multimedia application offering remote users an immersive video-conferencing environment. Kinect sensor, a consumer-grade range camera, facilitates the implementation of some recent 3D telepresence systems. However, when employing Kinect in 3D telepresence applications, we have to consider the following vital issues:</p>

<li> Raw Kinect depth data is noise-prone and unstable. We need proper filtering to improve the visual quality of 3D data in the 3D telepresence applications;</li>
<li> Existing data filtering methods on Kinect depth data mainly address uniformly-distributed random noise, but ignore depth quantization problem in the data; </li>
<li> In 3D telepresence applications, e.g., tele-conferencing, the hardware depth sensors have to be  mounted on a fixed  location  in the physical space, rather than being freely movable as in KinectFusion;</li>
<li> The filtering process should  run  in  real-time,  so  we cannot afford tedious computation with global data optimization, e.g., solving with Poisson equations. Considering above requirements, we propose a multi-scale direction-aware filtering method, aiming at making use of Kinect in 3D telepresence applications. Our method not only can effectively filter Kinect depth data, in particular  to resolve the depth quantization issue, but also can run in real-time by using GPUs. </li>
  <br>
 <b>Evaluation & Results</b>
<p><img src="/images/kinectfiltering-results.png" width="500" /> <p>
  <br>
 <b>Downloads</b>
<p><a href="/pdf/kinectfiltering-poster.pdf"> Poster </a> </p>
<a href="/pdf/kinectfiltering-paper.pdf"> Paper</a>
 
</body>
</html>
